---
title: "Untitled"
format: html
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## Introduction

This demo will be discussed on Day 3 (October 12, 2022) of the 2022 Machine Learning Workshop.

The Day 3 interactive hands-on session will illustrate the 10 machine learning best practices for building and deploying machine learning models that were discussed on Day 2.

-   [**Environment setup instructions**]{.underline}: If you registered for the workshop, you should have received an email with instructions for setting up your RStudio Cloud environment. If not, please contact Sydeaka Watson for more information.

-   [**Repository URL**]{.underline}: <https://github.com/sydeaka/machine-learning-workshop-2022>

## About Me

-   **Current role**: Senior Advisor - Innovative Sciences in SDnA/GSS

-   **Education**: Ph.D. Statistics, Baylor University, 2011

-   **Experience**:

    -   Biostatistician -- Los Alamos National Laboratory, University of Chicago Medicine

    -   Data Scentist -- AT&T, Elicit Insights (consulting firm), Eli Lilly (2020-present)

-   **Skills**: R / Python; Statistics; Machine Learning; Shiny app development

-   **Current residence**: Dallas, Texas

## Tips

In this demo, you will notice that I used the following principles to help me stay organized and focused. I highly encourage you to adopt these practices within your machine learning projects.

-   Use code for every step in the workflow (avoid manual steps if possible)

    -   data ingestion, data pre-processing, model fitting, data summaries / visualizations, final model selection, model deployment, etc.

-   Document your code.

    -   Use RMarkdown or Quarto to add annotations to code chunks and output

    -   Add lots of comments so that you understand what each chunk of code is doing

-   Create your own suite of helper functions that are stored in external scripts. This makes your code cleaner and easier to maintain.

    -   You could re-use your functions over multiple scenarios within the same project, or even across multiple projects.

    -   Code is cleaner, easier to read

-   Parameterize your code, so that you can easily/quickly adjust settings that are likely to change from one model training exercise to the next

-   Use the `logger` package to display descriptive logs as your workflow runs. This helps with troubleshooting if any issues arise. You could also use this package to save the logs associated with each training run to a log file.

-   To ensure that your results are reproducible (i.e., that you get the same results) from one run to the next, always use the `set.seed` function to set a random seed before each step that uses randomization. You will note that I always set the seed before I do any of the following:

    -   Split the data

    -   Create k-folds

    -   Train a model

# Setup

Workshop environment setup

```{r}
source(here::here("hands_on_day3", "day3_env_setup.R"))
```

ACTG175 demo environment setup

```{r}
source(here::here("hands_on_day3", "actg175_env_setup.R"))
source(here::here("hands_on_day3", "day3_model_utils.R"))
```

## Reminder: 10 ML best practices discussed on Day 2

1\. Define modeling objectives

2\. Exploratory data analysis

3\. Feature engineering

4\. Feature selection

5\. Split data into training / testing sets

6\. Train / validate the model

7\. Create visual summaries

8\. Explain the model predictions

9\. Deploy the model

10\. Create a ML workflow

# Apply 10 ML Best practices

## 1. Define modeling objectives

-   ACTG 175 was a randomized, double-blind, placebo-controlled trial that included HIV-1--infected subjects with CD4 cell counts 200-500 per cubic mm

-   Treatments

    -   Monotherapy: (1) zidovudine alone or (2) didanosine alone
    -   Combination therapy: (3) zidovudine + didanosine or (4) zidovudine + zalcitabine

-   Primary study end point: (1) \>50% decline in CD4 cell count, (2) progression to AIDS, or (3) death.

-   Objective: Given baseline characteristics, predict patient's CD8 level at \~20 weeks after randomization (`cd820`)

-   Outcome: Integer value representing the patient's CD8 level at \~20 weeks after randomization (`cd820`)

-   Data source: Clinical data -- baseline predictors, treatment assignment, study outcomes

-   Candidate algorithms

    -   Supervised learning / [**regression**]{.underline}: Random forest, Decision Tree, & Linear regression (Ordinary Least Squares, or OLS)

## 2. Exploratory data analysis

Load the ACTG175 data

-   In the helper function, notice that I'm getting the data stored in the package. Thus, the results will be identical each run (since the data aren't changing).
-   However, in practice, we would connect this to a data source that is refreshed at some frequency.
    -   For example, link to a table in a database
    -   We will get new model results each time we run the workflow as we train on newer, fresher datasets.

```{r}
logger::log_info("Run the ACTG175 data load and transformation pipeline")
proc_data <- actg175_load_and_transform_data()

logger::log_info("Unpack the data")
dat_raw <- proc_data$dat_raw
dat_clean <- proc_data$dat_clean
```

```{r}
logger::log_info("Preview the raw and processed data")
dat_raw %>% pretty_dt()
dat_clean %>% pretty_dt()
```

Exploratory data analysis: Show summaries and plots

```{r}
eda_results <- actg175_eda(dat_raw, dat_clean)
```

Missing data summary plot

-   The `cd496` outcome (CD4 count at \~96 weeks after treatment) is the only field that contains missing values.

```{r}
eda_results$summaries$missing_data_summary
```

Other summary plots

```{r}
eda_results$plot_list
```

## 3. Feature engineering

-   Create indicator variables for the individual drugs

-   Show the mapping between the indicators and the `treatment_arm` field

```{r feat_eng}
dat_clean <- dat_clean %>%
  mutate(
    ind_didanosine = ifelse(grepl(pattern='didanosine', x=treatment_arm), 1L, 0L),
    ind_zidovudine = ifelse(grepl(pattern='zidovudine', x=treatment_arm), 1L, 0L),
    ind_zalcitabine = ifelse(grepl(pattern='zalcitabine', x=treatment_arm), 1L, 0L)
  )

dat_clean %>%
  distinct(treatment_arm, ind_didanosine, ind_zidovudine, ind_zalcitabine) %>%
  pretty_dt
```

## 4. Feature selection

Decide which variables we might want to include in the model

```{r var_types}
logger::log_info("Identify fields to keep or exclude")
var_predictors <- c(
    # treatment assignment 
    'treatment_arm', 
    'ind_didanosine', 'ind_zidovudine', 'ind_zalcitabine',
    # baseline predictors
    'age', 'wtkg', 'hemo', 'lgbtq', 'hist_intra_drug_use', 'karnof', 'prior_z_30days', 
    'days_prior_art', 'race', 'gender', 'prior_art', 'strat_hist_art', 'symptom', 'baseline_cd4', 'baseline_cd8')

var_outcomes <- c('event', 'surv_event', 'surv_days', 'cd420', 'cd496', 'r', 'cd820')
vars_all <- colnames(dat_clean)

vars_to_keep <- c(var_predictors, var_outcomes)
vars_excluded <- vars_all[!(vars_all %in% vars_to_keep)]

logger::log_info("Get relevant fields")
dat_analysis <- dat_clean %>% select_at(vars_to_keep)

dat_analysis %>% summary
```

Specify the outcome that we will focus on in this modeling exercise

```{r}
selected_outcome <- "cd820"
logger::log_info("Fitting regression models predicting `{selected_outcome}`")
```

## 5. Split data into training / testing

### Initial data split

85 % training / validation + 15% test

```{r dat_split}
# Set the seed so the result is reproducible
set.seed(123)
logger::log_info("Initial data split with 85% training/validation + 15% testing")
dat_split <- initial_split(dat_analysis, prop = 0.85, strata = strat_hist_art)
dat_split
```

```{r dat_split_dfs}
logger::log_info("Creating train/val + testing dataframes")
dat_train_and_val <- training(dat_split)
dat_test <- testing(dat_split)
```

## 6. Train the model(s) + 7. Create visual summaries

Set up the parallel backend.

-   RStudio Cloud free account only provides a single core, so it will not be possible to leverage parallel processing on RStudio Cloud.

-   If you are running this demo outside of RStudio Cloud, you may uncomment the code in the chunk below.

-   Since we are using limited resources for this demo, you will note that the parameters have been adjusted so that we are using fewer trees, fewer cross-validation folds, and fewer iterations in the grid search. I also switched to models that don't require `RcppEigen` package since this is not readily supported in RStudio Cloud (unless you use their package manager... thanks, Eric!). To view a more realistic example of an ML workflow that uses more complex models and parallelization, see the demo linked below.

    -   <https://github.com/sydeaka/machine-learning-workshop-2022/blob/main/hands_on_day3/day3_actg175.qmd>

```{r parallel_setup}
# logger::log_info("Set up the parallel backend")
# cores <- parallelly::availableCores(logical = FALSE)
# cl <- parallel::makePSOCKcluster(cores)
# doParallel::registerDoParallel(cl)
```

Generate k folds for CV

-   Note the smaller number of folds that we are using in this limited RStudio Cloud environment.

-   Stratified fold split by the study stratification variable, history of anti-retroviral therapy

```{r}
set.seed(9265)
num_folds <- 2
logger::log_info("Using {num_folds}-fold validation stratified by history of anti-retroviral therapy")
dat_folds <- vfold_cv(dat_train_and_val, v = num_folds, strata = strat_hist_art)
```

Specify two model formulas.

-   First includes both representations of treatment arms

-   Second removes the binary indicators of the individual treatment

You may want to try it both ways to see if you get a different result.

```{r}
logger::log_info("Model formula 1 -- Treatment assignment variables: treatment_arm, ind_didanosine, ind_zidovudine, and ind_zalcitabine")
model_formula_all_pred <- as.formula(paste(selected_outcome, '~ ', paste(var_predictors, collapse = ' + ')))
model_formula_all_pred

logger::log_info("Model formula 2 -- Treatment assignment variables: treatment_arm only")
var_predictors_rm_trt_indicators <-  var_predictors[!(var_predictors %in% c('ind_didanosine', 'ind_zidovudine', 'ind_zalcitabine'))]
model_formula_rm_trt_indicators <- as.formula(paste(selected_outcome, ' ~ ', paste(var_predictors_rm_trt_indicators, collapse = ' + ')))

model_formula_rm_trt_indicators
```

Set control parameters.

-   Note the smaller grid size (5) that we are using in this limited RStudio Cloud environment.

-   `model_formula` parameter allows you to switch between the two model formulas specified in the previous chunk.

```{r}
logger::log_info("Set control parameters")
ctrl <- control_grid(save_pred = TRUE, allow_par=TRUE, parallel_over = "everything", 
                     verbose=TRUE, event_level='second')

grid_size <- 10

model_formula <- model_formula_rm_trt_indicators
```

Model specifications

-   Benchmark: OLS Linear model via the `lm` function implemented in the `stats` package
-   ML Models:
    -   Decision Tree model implemented in the `rpart` package

    -   Random Forest model implemented in the `randomForest` package
-   All Decision Tree and Random Forest parameters are will be tuned
-   Specifying parameter ranges for Decision Tree & Random Forest in later chunks
-   Thanks to tidymodels, we can leverage the same code structure as we advance from one algorithm to the next.

```{r}
logger::log_info("Create model specifications")

# Linear regression (benchmark model)
model_spec_lm <- linear_reg(mode = "regression", engine = "lm")

# Decision Tree
model_spec_decision_tree_tune <- decision_tree(mode = 'regression', engine = 'rpart', 
                             tree_depth = tune(), min_n = tune())

# Random forest
model_spec_rf_tune <- rand_forest(mode = 'regression', engine = 'randomForest', 
                             trees = tune(), mtry = tune(), min_n = tune())
```

### Fit benchmark model: OLS Linear regression with k-fold cross validation

-   We didn't specify which performance metrics we want to use, so we are using the two default performance metrics for regression models:

    -   Root mean squared error (rmse) -- lower values indicate higher performance

    -   R-squared (rsq) -- higher values indicate higher performance

-   Visualizing results of the model fits

    -   Scatterplot (predicted vs observed values) shows that this is a decent fit, but it isn't great.

    -   This aligns with the high RMSE and low R-squared values that also indicate the model's poor performance.

```{r}
logger::log_info("Fitting the benchmark model")

logger::log_info("OLS Linear: Model workflow")
lm_model_wflow <- workflow(model_formula, model_spec_lm)

logger::log_info("OLS Linear: Fit/evaluate the model using k-fold cross validation")
set.seed(9)
tm_lm <- system.time({
  lm_res <-
    lm_model_wflow %>%
    fit_resamples(dat_folds, control = ctrl)
})

# How long did it take to run?
elapsed_secs <- as.numeric(tm_lm['elapsed'])
elapsed_min <- elapsed_secs/60
logger::log_info("Training completed in {round(elapsed_secs,2)} seconds / {round(elapsed_min,2)} minutes.")

logger::log_info("OLS Linear: Select / show the top model configurations (best param combination)")
top_lm_model_config <- lm_res %>% 
  select_best(metric = 'rmse') %>%
  mutate(algorithm='OLS Linear')
top_lm_model_perf <- lm_res %>% 
  show_best(metric = 'rmse', n = 1) %>% 
  mutate(algorithm='OLS Linear')

logger::log_info("OLS Linear: Collect predictions over folds")
lm_preds <- lm_res %>% 
  collect_predictions() %>%
  filter(.config == top_lm_model_perf$.config)

logger::log_info("OLS Linear: Plot predicted vs observed")
plot_pred_vs_obs_scatter(preds=lm_preds,
                         plot_title="CV predictions: OLS Linear pred vs obs")

logger::log_info("OLS Linear: Summarize performance metrics over {num_folds} folds")
lm_metrics <- lm_res %>% 
  collect_metrics() %>%
  mutate(scenario = .config)

logger::log_info("OLS Linear: Plot performance metrics: mean +/- standard error")
plot_tune_grid_metrics(tune_metrics = lm_metrics)
```

### Hyperparameter tuning

#### Tune parameters in the Decision Tree model

-   Below, we are specifying the ranges for the `tree_depth` and `min_n` parameters

-   If we wanted to let the algorithm automatically set the ranges for the grid, we could set the `grid` parameter equal to an integer that represents the number of parameter combinations we want to try.

    -   See the commented out line below in the `tune_grid` function call for an example
    -   It is often helpful to use this as a starting point to get a preliminary understanding of the ranges we might want to explore for a new ML problem.

-   Visual summaries

    -   Scatterplot (predicted vs observed) shows that the fit still isn't great. This isn't surprising since we have such a small number of iterations in our grid due to limited computational resources in RStudio Cloud.

    -   If we had run it on a larger grid (i.e., with a larger value for the `grid_size` parameter, we would have a better idea of the relationship between performance and parameter values

    -   

```{r}
logger::log_info("Decision Tree: Model workflow")
decision_tree_model_wflow_tune <- workflow(model_formula, model_spec_decision_tree_tune)

logger::log_info("Decision Tree: Set ranges for the parameters we want to tune")
decision_tree_param_grid <- 
  decision_tree_model_wflow_tune %>% 
  extract_parameter_set_dials() %>% 
  update(
    tree_depth = tree_depth(c(5, 25)),
    min_n = min_n(c(25, 200))
  ) %>% 
  grid_max_entropy(size = grid_size)

logger::log_info("Decision Tree: Tune the hyperparameters using k-fold cross validation")
set.seed(9)
tm_decision_tree <- system.time({
  decision_tree_res <-
    decision_tree_model_wflow_tune %>%
    tune_grid(resamples = dat_folds, 
              grid = decision_tree_param_grid,
              #grid = grid_size,
              control = ctrl)
})

# How long did it take to run?
elapsed_secs <- as.numeric(tm_decision_tree['elapsed'])
elapsed_min <- elapsed_secs/60
logger::log_info("Training completed in {round(elapsed_secs,2)} seconds / {round(elapsed_min,2)} minutes.")

logger::log_info("Decision Tree: Select / show the top model configurations (best param combination)")
top_decision_tree_model_config <- decision_tree_res %>% 
  select_best(metric = 'rmse')  %>% 
  mutate(algorithm='Decision Tree')

top_decision_tree_model_perf <- decision_tree_res %>% 
  show_best(metric = 'rmse', n = 1) %>% 
  mutate(algorithm='Decision Tree')

logger::log_info("Decision Tree: Collect predictions over folds")
decision_tree_tune_preds <- decision_tree_res %>% 
  collect_predictions() %>%
  filter(.config == top_decision_tree_model_perf$.config)

logger::log_info("Decision Tree: Plot predicted vs observed")
plot_pred_vs_obs_scatter(preds=decision_tree_tune_preds,
                         plot_title="CV predictions: Decision Tree pred vs obs")

logger::log_info("Decision Tree: Summarize performance metrics over {num_folds} folds")
decision_tree_tune_metrics <- decision_tree_res %>% 
  collect_metrics() %>%
  mutate(scenario = .config)

logger::log_info("Decision Tree: Plot performance metrics: mean +/- standard error")
plot_tune_grid_metrics(tune_metrics = decision_tree_tune_metrics)

logger::log_info("Decision Tree: Plot performance metric vs parameter value")
plot_param(tune_metrics = decision_tree_tune_metrics, param = 'tree_depth', metric_type='rmse')
plot_param(tune_metrics = decision_tree_tune_metrics, param = 'min_n', metric_type='rmse')
```

#### Tune parameters in the Random Forest model

-   Below, we are specifying the ranges for the`trees`, `tree_depth` and `min_n` parameters

-   If we wanted to let the algorithm automatically set the ranges for the grid, we could set the `grid` parameter equal to an integer that represents the number of parameter combinations we want to try.

    -   See the commented out line below in the `tune_grid` function call for an example

-   Visual summaries

    -   Scatterplot (predicted vs observed) shows that the fit still isn't great. This isn't surprising since we have such a small number of iterations in our grid due to limited computational resources in RStudio Cloud.

    -   If we had run it on a larger grid (i.e., with a larger value for the `grid_size` parameter, we would have a better idea of the relationship between performance and parameter values

```{r}
logger::log_info("Random Forest: Model workflow")
rf_model_wflow_tune <- workflow(model_formula, model_spec_rf_tune)

logger::log_info("Random Forest: Set ranges for the parameters we want to tune")
rf_param_grid <- 
  rf_model_wflow_tune %>% 
  extract_parameter_set_dials() %>% 
  update(
    trees = trees(c(100, 300)),
    mtry = mtry(c(5, 20)),
    min_n = min_n(c(25, 200))
  ) %>% 
  grid_max_entropy(size = grid_size)

logger::log_info("Random Forest: Tune the hyperparameters using k-fold cross validation")
set.seed(9)
tm_rf <- system.time({
  rf_res <-
    rf_model_wflow_tune %>%
    tune_grid(resamples = dat_folds, 
              grid = rf_param_grid ,
              #grid = grid_size,
              control = ctrl)
})

# How long did it take to run?
elapsed_secs <- as.numeric(tm_rf['elapsed'])
elapsed_min <- elapsed_secs/60
logger::log_info("Training completed in {round(elapsed_secs,2)} seconds / {round(elapsed_min,2)} minutes.")

logger::log_info("Random Forest: Select / show the top model configurations (best param combination)")
top_rf_model_config <- rf_res %>% 
  select_best(metric = 'rmse')  %>% 
  mutate(algorithm='Random Forest')
top_rf_model_perf <- rf_res %>% 
  show_best(metric = 'rmse', n = 1) %>% 
  mutate(algorithm='Random Forest')

logger::log_info("Random Forest: Collect predictions over folds")
rf_tune_preds <- rf_res %>% 
  collect_predictions() %>%
  filter(.config == top_rf_model_perf$.config)

logger::log_info("Random Forest: Plot predicted vs observed")
plot_pred_vs_obs_scatter(preds=rf_tune_preds,
                         plot_title="CV predictions: Random Forest pred vs obs")

logger::log_info("Random Forest: Summarize performance metrics over {num_folds} folds")
rf_tune_metrics <- rf_res %>% 
  collect_metrics() %>%
  mutate(scenario = .config)

logger::log_info("Random Forest: Plot performance metrics: mean +/- standard error")
plot_tune_grid_metrics(tune_metrics = rf_tune_metrics)

logger::log_info("Random Forest: Plot performance metric vs parameter value")
plot_param(tune_metrics = rf_tune_metrics, param = 'trees', metric_type='rmse')
plot_param(tune_metrics = rf_tune_metrics, param = 'mtry', metric_type='rmse')
plot_param(tune_metrics = rf_tune_metrics, param = 'min_n', metric_type='rmse')
```

### Shut down the parallel backend

Uncomment if running outside of RStudio Cloud

```{r}
# logger::log_info("Shut down the parallel backend")
# foreach::registerDoSEQ()
# parallel::stopCluster(cl)
```

### Summarize parameter tuning results

These are all of the model workflows that we used in this demo

```{r}
model_workflows <- list("OLS Linear" = lm_model_wflow,
  "Random Forest" = rf_model_wflow_tune,
     "Decision Tree" = decision_tree_model_wflow_tune)
#model_workflows
```

Top models: Performance metrics in each algorithm

```{r}
model_perf <- list("OLS Linear" = top_lm_model_perf,
  "Random Forest" = top_rf_model_perf,
     "Decision Tree" = top_decision_tree_model_perf)
model_perf
```

Top models: Optimal hyperparameters in each algorithm

-   No parameters in the OLS Linear regression model since there were none that required tuning

```{r}
model_config <- list("OLS Linear" = top_lm_model_config,
  "Random Forest" = top_rf_model_config,
     "Decision Tree" = top_decision_tree_model_config)

model_config
```

Automatically determine the best model overall, i.e., the one that optimizes the RMSE performance metric

-   This is useful since the results will change from one run to the next

-   The user does not have to manually decide which model is the best one, i.e., which one should be deployed

-   Optimal hyperparameter values are automatically recorded in this `top_model_config` variable.

```{r}
rmse_vals <- sapply(model_perf, function(perf) perf$mean[1])
top_model_name <- which.min(rmse_vals) %>% names
top_model_config <- model_config[[top_model_name]]
logger::log_info("Top model: `{top_model_name}`")
top_model_config
```

Get the final workflow that applies the optimal parameters in the best model

```{r}
top_model_workflow <- model_workflows[[top_model_name]] %>%
  finalize_workflow(top_model_config)
```

After all of the training rounds, we are finally ready to generate the final model fit

-   This one trains the model on the the combined training/validation dataset stored in the `dat_train_and_val` dataframe

-   Final model performance is evaluated on the held-out `dat_test` dataframe

```{r}
top_model_fit <- 
  top_model_workflow %>% 
  last_fit(split = dat_split)
```

Get the workflow associated with the trained model

```{r}
top_model_fit_wflow <- top_model_fit %>% extract_workflow()
```

We could alternatively extract the fitted object in its native format.

-   For example, this would return a `lm` class object that can be used with native `stats` package utilities as if you had fit it directly outside of the tidymodels framework.

-   We don't need this for this demo. I just wanted to point this out in case you need it for your project.

```{r}
top_model_obj <- top_model_fit %>% extract_fit_engine()
class(top_model_obj)
top_model_obj
```

## 8. Explain the model predictions

Sorry, I didn't have time to finish the model explainability example in time for this demo. We can revisit this in a future session.

## 9. Deploy the model

This code has been adapted from the DC Bikeshare demo provided by RStudio

-   DC Bikeshare Demo: <https://solutions.rstudio.com/example/bike_predict/>

-   Model training / deployment code: <https://colorado.rstudio.com/rsc/bike-predict-r-train-and-deploy-model/document.html>

First, we create a vetiver model object.

```{r}
# Create a model name and a pin name
model_name <- glue::glue("actg175_{selected_outcome}_model_predict")
pin_name <- glue::glue("{Sys.getenv('CONNECT_USER_ID')}/{model_name}")

# Model metadata
model_metadata <- list(
  date_model_trained = as.character(Sys.Date()),
  top_model_name = top_model_name,
  top_model_config = top_model_config %>% as.list
)

print(model_metadata)
# $date_model_trained
# [1] "2022-10-11"
# 
# $top_model_name
# [1] "OLS Linear"
# 
# $top_model_config
# $top_model_config$.config
# [1] "Preprocessor1_Model1"
# 
# $top_model_config$algorithm
# [1] "OLS Linear"


# Create the vetiver model.
v <- vetiver_model(
  top_model_fit_wflow, 
  model_name,
  versioned = TRUE,
  save_ptype = dat_train_and_val %>%
    head(1) %>%
    select_at(var_predictors),
  metadata = model_metadata
)

v
# ── actg175_cd820_model_predict ─ <bundled_workflow> model for deployment 
# A lm regression modeling workflow using 19 features
```

Save the model as a pin to RStudio Connect:

```{r}
# Use RStudio Connect as a board.
board <- pins::board_rsconnect(
  server = Sys.getenv("CONNECT_SERVER"),
  key = Sys.getenv("CONNECT_API_KEY"),
  versioned = TRUE
)

# View the board before making any changes
board
```


```{r}
# Write the model to the board.
board %>%
 vetiver_pin_write(vetiver_model = v)

# View the board
board
```


Convert the model into a plumber API. The function vetiver_write_plumber will generate the plumber code for you and write it to plumber.R.

```{r}
# Write the model to `api/plumber.R`.
api_folder <- here::here("hands_on_day3", "api")
if (!dir.exists(api_folder)) dir.create(api_folder)
vetiver_write_plumber(board, pin_name, file = file.path(api_folder, "plumber.R"))

# Write a manifest.json file for the api
rsconnect::writeManifest(api_folder)
```

Then, deploy the plumber API to RStudio Connect.

```{r}
app_name <- "actg175-cd4-20wk-model-predict"
app_title <- "ACTG175 CD4 20 wk - Model - API"

# Establish a connection to RStudio connect.
client <- connectapi::connect(
  server = Sys.getenv("CONNECT_SERVER"),
  api_key = Sys.getenv("CONNECT_API_KEY")
)

# Deploy the content.
content <- 
  client %>%
  connectapi::deploy(
    connectapi::bundle_dir(api_folder),
    name = app_name,
    title = app_title
  )

content
```

This code was provided in the demo, but it was commented out. My model was deployed after running the previous code, so I'm not sure what this does.

```{r}
# # Deploy the content.
# vetiver_deploy_rsconnect(
#  board = board,
#  name = pin_name,
#  version = NULL,
#  appTitle = app_title,
#  account = Sys.getenv("CONNECT_USER_ID")
# )
```

View the dashboard URL. You can view this board if you'd like to view the Vetiver help page that was automatically generated for you for your model deployment.

```{r}
content$get_dashboard_url()
```

Get results from the API

```{bash}
curl -X POST "${CONNECT_SERVER}/content/a4d4bf54-dbaa-4f58-af21-d0fba64416f0/predict" \
 -H "Accept: application/json" \
 -H "Content-Type: application/json" \
 -H "Authorization: Key ${CONNECT_API_KEY}" \
 -d '[{"treatment_arm":"zidovudine and zalcitabine","ind_didanosine":0,"ind_zidovudine":1,"ind_zalcitabine":1,"age":36,"wtkg":81.6,"hemo":"no","lgbtq":"yes","hist_intra_drug_use":"yes","karnof":100,"prior_z_30days":"yes","days_prior_art":86,"race":"white","gender":"male","prior_art":"experienced","strat_hist_art":"> 1 but <= 52 weeks of prior antiretroviral therapy","symptom":"symptomatic","baseline_cd4":236,"baseline_cd8":634}]' 
```

You can also use the convenient utilities in the R `httr` package to make calls against the model API

```{r}
library(httr)

connectServer <- Sys.getenv("CONNECT_SERVER")
connectAPIKey <- Sys.getenv("CONNECT_API_KEY")

data_input_json <- '{"treatment_arm":"zidovudine and zalcitabine","ind_didanosine":0,"ind_zidovudine":1,"ind_zalcitabine":1,"age":36,"wtkg":81.6,"hemo":"no","lgbtq":"yes","hist_intra_drug_use":"yes","karnof":100,"prior_z_30days":"yes","days_prior_art":86,"race":"white","gender":"male","prior_art":"experienced","strat_hist_art":"> 1 but <= 52 weeks of prior antiretroviral therapy","symptom":"symptomatic","baseline_cd4":236,"baseline_cd8":634}'

resp <- httr::POST(
  connectServer, 
     # You will need to change this to path associated with your own model API
    path = "/content/a4d4bf54-dbaa-4f58-af21-d0fba64416f0/predict", 
    body = data_input_json,
    encode = "raw",
    add_headers(
      Accept = "application/json",
      `Content-Type` = "application/json",
      Authorization = paste0("Key ", connectAPIKey)),
)


result <- httr::content(resp, as = "parsed")
result
```


## 10. Create a ML workflow

Discussed in a future session

# Closing thoughts

## Closing thoughts

-   In my final email to you, I will send a link to the video recording for Day 3
-   I will also send a link to a survey. Please fill it out and share any feedback you may have about the course content.
-   In the survey, I will ask you to let me know if there are any additional machine learning topics that you'd like to learn more about in future sessions. For example, these may include:
    -   Python-based hands-on workshop
    -   Model explainability
    -   Unsupervised learning (for example, clustering/segmentation)
    -   Machine learning workflow with targets (Will Landau)
    -   Other topics!
-   There are plans for another AI/ML workshop this fall. Please reach out to Haoda Fu for details, or follow the Community of Practice for the latest updates about the workshop.
-   Thank you for attending!
